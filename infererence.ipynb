{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    \"\"\"Read a JSON file and parse it into a Python object.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file to read.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary object representing the JSON data.\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified file path does not exist.\n",
    "        ValueError: If the specified file path does not contain valid JSON data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the file and load the JSON data into a Python object\n",
    "        with open(file_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "        return json_data\n",
    "    except FileNotFoundError:\n",
    "        # Raise an error if the file path does not exist\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    except ValueError:\n",
    "        # Raise an error if the file does not contain valid JSON data\n",
    "        raise ValueError(f\"Invalid JSON data in file: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "... LOAD SIGN TO PREDICTION INDEX MAP FROM JSON FILE ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "print(\"\\n\\n... LOAD SIGN TO PREDICTION INDEX MAP FROM JSON FILE ...\\n\")\n",
    "s2p_map = {k.lower():v for k,v in read_json_file(os.path.join(\"sign_to_prediction_index_map.json\")).items()}\n",
    "p2s_map = {v:k for k,v in read_json_file(os.path.join(\"sign_to_prediction_index_map.json\")).items()}\n",
    "encoder = lambda x: s2p_map.get(x.lower())\n",
    "decoder = lambda x: p2s_map.get(x)\n",
    "print(s2p_map)\n",
    "train_df['label'] = train_df.sign.map(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'TV',\n",
       " 1: 'after',\n",
       " 2: 'airplane',\n",
       " 3: 'all',\n",
       " 4: 'alligator',\n",
       " 5: 'animal',\n",
       " 6: 'another',\n",
       " 7: 'any',\n",
       " 8: 'apple',\n",
       " 9: 'arm',\n",
       " 10: 'aunt',\n",
       " 11: 'awake',\n",
       " 12: 'backyard',\n",
       " 13: 'bad',\n",
       " 14: 'balloon',\n",
       " 15: 'bath',\n",
       " 16: 'because',\n",
       " 17: 'bed',\n",
       " 18: 'bedroom',\n",
       " 19: 'bee',\n",
       " 20: 'before',\n",
       " 21: 'beside',\n",
       " 22: 'better',\n",
       " 23: 'bird',\n",
       " 24: 'black',\n",
       " 25: 'blow',\n",
       " 26: 'blue',\n",
       " 27: 'boat',\n",
       " 28: 'book',\n",
       " 29: 'boy',\n",
       " 30: 'brother',\n",
       " 31: 'brown',\n",
       " 32: 'bug',\n",
       " 33: 'bye',\n",
       " 34: 'callonphone',\n",
       " 35: 'can',\n",
       " 36: 'car',\n",
       " 37: 'carrot',\n",
       " 38: 'cat',\n",
       " 39: 'cereal',\n",
       " 40: 'chair',\n",
       " 41: 'cheek',\n",
       " 42: 'child',\n",
       " 43: 'chin',\n",
       " 44: 'chocolate',\n",
       " 45: 'clean',\n",
       " 46: 'close',\n",
       " 47: 'closet',\n",
       " 48: 'cloud',\n",
       " 49: 'clown',\n",
       " 50: 'cow',\n",
       " 51: 'cowboy',\n",
       " 52: 'cry',\n",
       " 53: 'cut',\n",
       " 54: 'cute',\n",
       " 55: 'dad',\n",
       " 56: 'dance',\n",
       " 57: 'dirty',\n",
       " 58: 'dog',\n",
       " 59: 'doll',\n",
       " 60: 'donkey',\n",
       " 61: 'down',\n",
       " 62: 'drawer',\n",
       " 63: 'drink',\n",
       " 64: 'drop',\n",
       " 65: 'dry',\n",
       " 66: 'dryer',\n",
       " 67: 'duck',\n",
       " 68: 'ear',\n",
       " 69: 'elephant',\n",
       " 70: 'empty',\n",
       " 71: 'every',\n",
       " 72: 'eye',\n",
       " 73: 'face',\n",
       " 74: 'fall',\n",
       " 75: 'farm',\n",
       " 76: 'fast',\n",
       " 77: 'feet',\n",
       " 78: 'find',\n",
       " 79: 'fine',\n",
       " 80: 'finger',\n",
       " 81: 'finish',\n",
       " 82: 'fireman',\n",
       " 83: 'first',\n",
       " 84: 'fish',\n",
       " 85: 'flag',\n",
       " 86: 'flower',\n",
       " 87: 'food',\n",
       " 88: 'for',\n",
       " 89: 'frenchfries',\n",
       " 90: 'frog',\n",
       " 91: 'garbage',\n",
       " 92: 'gift',\n",
       " 93: 'giraffe',\n",
       " 94: 'girl',\n",
       " 95: 'give',\n",
       " 96: 'glasswindow',\n",
       " 97: 'go',\n",
       " 98: 'goose',\n",
       " 99: 'grandma',\n",
       " 100: 'grandpa',\n",
       " 101: 'grass',\n",
       " 102: 'green',\n",
       " 103: 'gum',\n",
       " 104: 'hair',\n",
       " 105: 'happy',\n",
       " 106: 'hat',\n",
       " 107: 'hate',\n",
       " 108: 'have',\n",
       " 109: 'haveto',\n",
       " 110: 'head',\n",
       " 111: 'hear',\n",
       " 112: 'helicopter',\n",
       " 113: 'hello',\n",
       " 114: 'hen',\n",
       " 115: 'hesheit',\n",
       " 116: 'hide',\n",
       " 117: 'high',\n",
       " 118: 'home',\n",
       " 119: 'horse',\n",
       " 120: 'hot',\n",
       " 121: 'hungry',\n",
       " 122: 'icecream',\n",
       " 123: 'if',\n",
       " 124: 'into',\n",
       " 125: 'jacket',\n",
       " 126: 'jeans',\n",
       " 127: 'jump',\n",
       " 128: 'kiss',\n",
       " 129: 'kitty',\n",
       " 130: 'lamp',\n",
       " 131: 'later',\n",
       " 132: 'like',\n",
       " 133: 'lion',\n",
       " 134: 'lips',\n",
       " 135: 'listen',\n",
       " 136: 'look',\n",
       " 137: 'loud',\n",
       " 138: 'mad',\n",
       " 139: 'make',\n",
       " 140: 'man',\n",
       " 141: 'many',\n",
       " 142: 'milk',\n",
       " 143: 'minemy',\n",
       " 144: 'mitten',\n",
       " 145: 'mom',\n",
       " 146: 'moon',\n",
       " 147: 'morning',\n",
       " 148: 'mouse',\n",
       " 149: 'mouth',\n",
       " 150: 'nap',\n",
       " 151: 'napkin',\n",
       " 152: 'night',\n",
       " 153: 'no',\n",
       " 154: 'noisy',\n",
       " 155: 'nose',\n",
       " 156: 'not',\n",
       " 157: 'now',\n",
       " 158: 'nuts',\n",
       " 159: 'old',\n",
       " 160: 'on',\n",
       " 161: 'open',\n",
       " 162: 'orange',\n",
       " 163: 'outside',\n",
       " 164: 'owie',\n",
       " 165: 'owl',\n",
       " 166: 'pajamas',\n",
       " 167: 'pen',\n",
       " 168: 'pencil',\n",
       " 169: 'penny',\n",
       " 170: 'person',\n",
       " 171: 'pig',\n",
       " 172: 'pizza',\n",
       " 173: 'please',\n",
       " 174: 'police',\n",
       " 175: 'pool',\n",
       " 176: 'potty',\n",
       " 177: 'pretend',\n",
       " 178: 'pretty',\n",
       " 179: 'puppy',\n",
       " 180: 'puzzle',\n",
       " 181: 'quiet',\n",
       " 182: 'radio',\n",
       " 183: 'rain',\n",
       " 184: 'read',\n",
       " 185: 'red',\n",
       " 186: 'refrigerator',\n",
       " 187: 'ride',\n",
       " 188: 'room',\n",
       " 189: 'sad',\n",
       " 190: 'same',\n",
       " 191: 'say',\n",
       " 192: 'scissors',\n",
       " 193: 'see',\n",
       " 194: 'shhh',\n",
       " 195: 'shirt',\n",
       " 196: 'shoe',\n",
       " 197: 'shower',\n",
       " 198: 'sick',\n",
       " 199: 'sleep',\n",
       " 200: 'sleepy',\n",
       " 201: 'smile',\n",
       " 202: 'snack',\n",
       " 203: 'snow',\n",
       " 204: 'stairs',\n",
       " 205: 'stay',\n",
       " 206: 'sticky',\n",
       " 207: 'store',\n",
       " 208: 'story',\n",
       " 209: 'stuck',\n",
       " 210: 'sun',\n",
       " 211: 'table',\n",
       " 212: 'talk',\n",
       " 213: 'taste',\n",
       " 214: 'thankyou',\n",
       " 215: 'that',\n",
       " 216: 'there',\n",
       " 217: 'think',\n",
       " 218: 'thirsty',\n",
       " 219: 'tiger',\n",
       " 220: 'time',\n",
       " 221: 'tomorrow',\n",
       " 222: 'tongue',\n",
       " 223: 'tooth',\n",
       " 224: 'toothbrush',\n",
       " 225: 'touch',\n",
       " 226: 'toy',\n",
       " 227: 'tree',\n",
       " 228: 'uncle',\n",
       " 229: 'underwear',\n",
       " 230: 'up',\n",
       " 231: 'vacuum',\n",
       " 232: 'wait',\n",
       " 233: 'wake',\n",
       " 234: 'water',\n",
       " 235: 'wet',\n",
       " 236: 'weus',\n",
       " 237: 'where',\n",
       " 238: 'white',\n",
       " 239: 'who',\n",
       " 240: 'why',\n",
       " 241: 'will',\n",
       " 242: 'wolf',\n",
       " 243: 'yellow',\n",
       " 244: 'yes',\n",
       " 245: 'yesterday',\n",
       " 246: 'yourself',\n",
       " 247: 'yucky',\n",
       " 248: 'zebra',\n",
       " 249: 'zipper'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2s_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sign</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_landmark_files/26734/1000035562.parquet</td>\n",
       "      <td>26734</td>\n",
       "      <td>1000035562</td>\n",
       "      <td>blow</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_landmark_files/28656/1000106739.parquet</td>\n",
       "      <td>28656</td>\n",
       "      <td>1000106739</td>\n",
       "      <td>wait</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_landmark_files/16069/100015657.parquet</td>\n",
       "      <td>16069</td>\n",
       "      <td>100015657</td>\n",
       "      <td>cloud</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_landmark_files/25571/1000210073.parquet</td>\n",
       "      <td>25571</td>\n",
       "      <td>1000210073</td>\n",
       "      <td>bird</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_landmark_files/62590/1000240708.parquet</td>\n",
       "      <td>62590</td>\n",
       "      <td>1000240708</td>\n",
       "      <td>owie</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94472</th>\n",
       "      <td>train_landmark_files/53618/999786174.parquet</td>\n",
       "      <td>53618</td>\n",
       "      <td>999786174</td>\n",
       "      <td>white</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94473</th>\n",
       "      <td>train_landmark_files/26734/999799849.parquet</td>\n",
       "      <td>26734</td>\n",
       "      <td>999799849</td>\n",
       "      <td>have</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94474</th>\n",
       "      <td>train_landmark_files/25571/999833418.parquet</td>\n",
       "      <td>25571</td>\n",
       "      <td>999833418</td>\n",
       "      <td>flower</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94475</th>\n",
       "      <td>train_landmark_files/29302/999895257.parquet</td>\n",
       "      <td>29302</td>\n",
       "      <td>999895257</td>\n",
       "      <td>room</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94476</th>\n",
       "      <td>train_landmark_files/36257/999962374.parquet</td>\n",
       "      <td>36257</td>\n",
       "      <td>999962374</td>\n",
       "      <td>happy</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94477 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  participant_id  \\\n",
       "0      train_landmark_files/26734/1000035562.parquet           26734   \n",
       "1      train_landmark_files/28656/1000106739.parquet           28656   \n",
       "2       train_landmark_files/16069/100015657.parquet           16069   \n",
       "3      train_landmark_files/25571/1000210073.parquet           25571   \n",
       "4      train_landmark_files/62590/1000240708.parquet           62590   \n",
       "...                                              ...             ...   \n",
       "94472   train_landmark_files/53618/999786174.parquet           53618   \n",
       "94473   train_landmark_files/26734/999799849.parquet           26734   \n",
       "94474   train_landmark_files/25571/999833418.parquet           25571   \n",
       "94475   train_landmark_files/29302/999895257.parquet           29302   \n",
       "94476   train_landmark_files/36257/999962374.parquet           36257   \n",
       "\n",
       "       sequence_id    sign  label  \n",
       "0       1000035562    blow     25  \n",
       "1       1000106739    wait    232  \n",
       "2        100015657   cloud     48  \n",
       "3       1000210073    bird     23  \n",
       "4       1000240708    owie    164  \n",
       "...            ...     ...    ...  \n",
       "94472    999786174   white    238  \n",
       "94473    999799849    have    108  \n",
       "94474    999833418  flower     86  \n",
       "94475    999895257    room    188  \n",
       "94476    999962374   happy    105  \n",
       "\n",
       "[94477 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "708\n"
     ]
    }
   ],
   "source": [
    "ROWS_PER_FRAME = 543\n",
    "MAX_LEN = 384\n",
    "CROP_LEN = MAX_LEN\n",
    "NUM_CLASSES  = 250\n",
    "PAD = -100.\n",
    "NOSE=[\n",
    "    1,2,98,327\n",
    "]\n",
    "LNOSE = [98]\n",
    "RNOSE = [327]\n",
    "LIP = [ 0, \n",
    "    61, 185, 40, 39, 37, 267, 269, 270, 409,\n",
    "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "]\n",
    "LLIP = [84,181,91,146,61,185,40,39,37,87,178,88,95,78,191,80,81,82]\n",
    "RLIP = [314,405,321,375,291,409,270,269,267,317,402,318,324,308,415,310,311,312]\n",
    "\n",
    "POSE = [500, 502, 504, 501, 503, 505, 512, 513]\n",
    "LPOSE = [513,505,503,501]\n",
    "RPOSE = [512,504,502,500]\n",
    "\n",
    "REYE = [\n",
    "    33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
    "    246, 161, 160, 159, 158, 157, 173,\n",
    "]\n",
    "LEYE = [\n",
    "    263, 249, 390, 373, 374, 380, 381, 382, 362,\n",
    "    466, 388, 387, 386, 385, 384, 398,\n",
    "]\n",
    "\n",
    "LHAND = np.arange(468, 489).tolist()\n",
    "RHAND = np.arange(522, 543).tolist()\n",
    "\n",
    "POINT_LANDMARKS = LIP + LHAND + RHAND + NOSE + REYE + LEYE #+POSE\n",
    "\n",
    "NUM_NODES = len(POINT_LANDMARKS)\n",
    "CHANNELS = 6*NUM_NODES\n",
    "\n",
    "print(NUM_NODES)\n",
    "print(CHANNELS)\n",
    "\n",
    "def tf_nan_mean(x, axis=0, keepdims=False):\n",
    "    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims)\n",
    "\n",
    "def tf_nan_std(x, center=None, axis=0, keepdims=False):\n",
    "    if center is None:\n",
    "        center = tf_nan_mean(x, axis=axis,  keepdims=True)\n",
    "    d = x - center\n",
    "    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n",
    "\n",
    "class Preprocess(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len=MAX_LEN, point_landmarks=POINT_LANDMARKS, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_len = max_len\n",
    "        self.point_landmarks = point_landmarks\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if tf.rank(inputs) == 3:\n",
    "            x = inputs[None,...]\n",
    "        else:\n",
    "            x = inputs\n",
    "        \n",
    "        mean = tf_nan_mean(tf.gather(x, [17], axis=2), axis=[1,2], keepdims=True)\n",
    "        mean = tf.where(tf.math.is_nan(mean), tf.constant(0.5,x.dtype), mean)\n",
    "        x = tf.gather(x, self.point_landmarks, axis=2) #N,T,P,C\n",
    "        std = tf_nan_std(x, center=mean, axis=[1,2], keepdims=True)\n",
    "        \n",
    "        x = (x - mean)/std\n",
    "\n",
    "        if self.max_len is not None:\n",
    "            x = x[:,:self.max_len]\n",
    "        length = tf.shape(x)[1]\n",
    "        x = x[...,:2]\n",
    "\n",
    "        dx = tf.cond(tf.shape(x)[1]>1,lambda:tf.pad(x[:,1:] - x[:,:-1], [[0,0],[0,1],[0,0],[0,0]]),lambda:tf.zeros_like(x))\n",
    "\n",
    "        dx2 = tf.cond(tf.shape(x)[1]>2,lambda:tf.pad(x[:,2:] - x[:,:-2], [[0,0],[0,2],[0,0],[0,0]]),lambda:tf.zeros_like(x))\n",
    "\n",
    "        x = tf.concat([\n",
    "            tf.reshape(x, (-1,length,2*len(self.point_landmarks))),\n",
    "            tf.reshape(dx, (-1,length,2*len(self.point_landmarks))),\n",
    "            tf.reshape(dx2, (-1,length,2*len(self.point_landmarks))),\n",
    "        ], axis = -1)\n",
    "        \n",
    "        x = tf.where(tf.math.is_nan(x),tf.constant(0.,x.dtype),x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECA(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n",
    "        nn = tf.expand_dims(nn, -1)\n",
    "        nn = self.conv(nn)\n",
    "        nn = tf.squeeze(nn, -1)\n",
    "        nn = tf.nn.sigmoid(nn)\n",
    "        nn = nn[:,None,:]\n",
    "        return inputs * nn\n",
    "\n",
    "class LateDropout(tf.keras.layers.Layer):\n",
    "    def __init__(self, rate, noise_shape=None, start_step=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.rate = rate\n",
    "        self.start_step = start_step\n",
    "        self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n",
    "      \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n",
    "        self._train_counter = tf.Variable(0, dtype=\"int64\", aggregation=agg, trainable=False)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = tf.cond(self._train_counter < self.start_step, lambda:inputs, lambda:self.dropout(inputs, training=training))\n",
    "        if training:\n",
    "            self._train_counter.assign_add(1)\n",
    "        return x\n",
    "\n",
    "class CausalDWConv1D(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "        kernel_size=17,\n",
    "        dilation_rate=1,\n",
    "        use_bias=False,\n",
    "        depthwise_initializer='glorot_uniform',\n",
    "        name='', **kwargs):\n",
    "        super().__init__(name=name,**kwargs)\n",
    "        self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0),name=name + '_pad')\n",
    "        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n",
    "                            kernel_size,\n",
    "                            strides=1,\n",
    "                            dilation_rate=dilation_rate,\n",
    "                            padding='valid',\n",
    "                            use_bias=use_bias,\n",
    "                            depthwise_initializer=depthwise_initializer,\n",
    "                            name=name + '_dwconv')\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.causal_pad(inputs)\n",
    "        x = self.dw_conv(x)\n",
    "        return x\n",
    "\n",
    "def Conv1DBlock(channel_size,\n",
    "          kernel_size,\n",
    "          dilation_rate=1,\n",
    "          drop_rate=0.0,\n",
    "          expand_ratio=2,\n",
    "          se_ratio=0.25,\n",
    "          activation='swish',\n",
    "          name=None):\n",
    "    '''\n",
    "    efficient conv1d block, @hoyso48\n",
    "    '''\n",
    "    if name is None:\n",
    "        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n",
    "    # Expansion phase\n",
    "    def apply(inputs):\n",
    "        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n",
    "        channels_expand = channels_in * expand_ratio\n",
    "\n",
    "        skip = inputs\n",
    "\n",
    "        x = tf.keras.layers.Dense(\n",
    "            channels_expand,\n",
    "            use_bias=True,\n",
    "            activation=activation,\n",
    "            name=name + '_expand_conv')(inputs)\n",
    "\n",
    "        # Depthwise Convolution\n",
    "        x = CausalDWConv1D(kernel_size,\n",
    "            dilation_rate=dilation_rate,\n",
    "            use_bias=False,\n",
    "            name=name + '_dwconv')(x)\n",
    "\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn')(x)\n",
    "\n",
    "        x  = ECA()(x)\n",
    "\n",
    "        x = tf.keras.layers.Dense(\n",
    "            channel_size,\n",
    "            use_bias=True,\n",
    "            name=name + '_project_conv')(x)\n",
    "\n",
    "        if drop_rate > 0:\n",
    "            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop')(x)\n",
    "\n",
    "        if (channels_in == channel_size):\n",
    "            x = tf.keras.layers.add([x, skip], name=name + '_add')\n",
    "        return x\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.scale = self.dim ** -0.5\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n",
    "        self.drop1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        qkv = self.qkv(inputs)\n",
    "        qkv = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv))\n",
    "        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n",
    "\n",
    "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, None, :]\n",
    "\n",
    "        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n",
    "        attn = self.drop1(attn)\n",
    "\n",
    "        x = attn @ v\n",
    "        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def TransformerBlock(dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation='swish'):\n",
    "    def apply(inputs):\n",
    "        x = inputs\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n",
    "        x = MultiHeadSelfAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout)(x)\n",
    "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
    "        x = tf.keras.layers.Add()([inputs, x])\n",
    "        attn_out = x\n",
    "\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n",
    "        x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation)(x)\n",
    "        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n",
    "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
    "        x = tf.keras.layers.Add()([attn_out, x])\n",
    "        return x\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(max_len=MAX_LEN, dropout_step=0, dim=192):\n",
    "    inp = tf.keras.Input((max_len,CHANNELS))\n",
    "    #x = tf.keras.layers.Masking(mask_value=PAD,input_shape=(max_len,CHANNELS))(inp) #we don't need masking layer with inference\n",
    "    x = inp\n",
    "    ksize = 17\n",
    "    x = tf.keras.layers.Dense(dim, use_bias=False,name='stem_conv')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(momentum=0.95,name='stem_bn')(x)\n",
    "\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = TransformerBlock(dim,expand=2)(x)\n",
    "\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = TransformerBlock(dim,expand=2)(x)\n",
    "\n",
    "    if dim == 384: #for the 4x sized model\n",
    "        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "        x = TransformerBlock(dim,expand=2)(x)\n",
    "\n",
    "        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "        x = TransformerBlock(dim,expand=2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(dim*2,activation=None,name='top_conv')(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = LateDropout(0.8, start_step=dropout_step)(x)\n",
    "    x = tf.keras.layers.Dense(NUM_CLASSES,name='classifier')(x)\n",
    "    return tf.keras.Model(inp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = [\n",
    "                'model_weights/islr-models/islr-fp16-192-8-seed42-foldall-last.h5', #comment out other weights to check single model score\n",
    "            #    'model_weights/islr-models/islr-fp16-192-8-seed43-foldall-last.h5',\n",
    "            #    'model_weights/islr-models/islr-fp16-192-8-seed44-foldall-last.h5',\n",
    "               #'/kaggle/input/islr-models/islr-fp16-192-8-seed45-foldall-last.h5',\n",
    "              ]\n",
    "models = [get_model() for _ in models_path]\n",
    "for model,path in zip(models,models_path):\n",
    "    model.load_weights(path)\n",
    "models[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFLiteModel(tf.Module):\n",
    "    \"\"\"\n",
    "    TensorFlow Lite model that takes input tensors and applies:\n",
    "        – a preprocessing model\n",
    "        – the ISLR model \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, islr_models):\n",
    "        \"\"\"\n",
    "        Initializes the TFLiteModel with the specified preprocessing model and ISLR model.\n",
    "        \"\"\"\n",
    "        super(TFLiteModel, self).__init__()\n",
    "\n",
    "        # Load the feature generation and main models\n",
    "        self.prep_inputs = Preprocess()\n",
    "        self.islr_models   = islr_models\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 543, 3], dtype=tf.float32, name='inputs')])\n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"\n",
    "        Applies the feature generation model and main model to the input tensors.\n",
    "\n",
    "        Args:\n",
    "            inputs: Input tensor with shape [batch_size, 543, 3].\n",
    "\n",
    "        Returns:\n",
    "            A dictionary with a single key 'outputs' and corresponding output tensor.\n",
    "        \"\"\"\n",
    "        x = self.prep_inputs(tf.cast(inputs, dtype=tf.float32))\n",
    "        outputs = [model(x) for model in self.islr_models]\n",
    "        outputs = tf.keras.layers.Average()(outputs)[0]\n",
    "        return {'outputs': outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet('/kaggle/input/asl-signs/' + pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_keras_model = TFLiteModel(islr_models=models)\n",
    "demo_output = tflite_keras_model(load_relevant_data_subset(train_df.path[0]))[\"outputs\"]\n",
    "decoder(np.argmax(demo_output.numpy(), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflite_keras_model)\n",
    "keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "keras_model_converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = keras_model_converter.convert()\n",
    "with open('model_weights/model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "# !zip submission.zip /kaggle/working/model.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check inference time\n",
    "#code from @hengck23\n",
    "mode = 's' #'d'ebug #'s'ubmit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "if mode in ['d']:  \n",
    "    try:\n",
    "        import tflite_runtime\n",
    "    except:\n",
    "        !pip install tflite-runtime\n",
    "\n",
    "    import tflite_runtime.interpreter as tflite   \n",
    "    import tflite_runtime\n",
    "    print(tflite_runtime.__version__)\n",
    "\n",
    "print('import ok')\n",
    "'''\n",
    "Your model must also require less than 40 MB in memory and \n",
    "perform inference with less than 100 milliseconds of latency per video. \n",
    "Expect to see approximately 40,000 videos in the test set. \n",
    "We allow an additional 10 minute buffer for loading the data and miscellaneous overhead.\n",
    "\n",
    "'''\n",
    "def time_to_str(t, mode='min'):\n",
    "    if mode=='min':\n",
    "        t  = int(t)/60\n",
    "        hr = t//60\n",
    "        min = t%60\n",
    "        return '%2d hr %02d min'%(hr,min)\n",
    "\n",
    "    elif mode=='sec':\n",
    "        t   = int(t)\n",
    "        min = t//60\n",
    "        sec = t%60\n",
    "        return '%2d min %02d sec'%(min,sec)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "ROWS_PER_FRAME = 543\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "if mode in ['d']: \n",
    " \n",
    "    interpreter = tflite.Interpreter('/kaggle/working/model.tflite')\n",
    "    prediction_fn = interpreter.get_signature_runner('serving_default')\n",
    "#     valid_df = pd.read_csv('/kaggle/input/asl-demo/train_prepared.csv') \n",
    "#     valid_df = train_df[train_df.fold==0].reset_index(drop=True)\n",
    "#     valid_df = valid_df[:1000]\n",
    "    valid_df = train_df[:1000]\n",
    "    valid_num = len(valid_df)\n",
    "    valid = {\n",
    "        'sign':[],\n",
    "    }\n",
    "\n",
    "    start_timer = timer()\n",
    "    for t, d in valid_df.iterrows():\n",
    "\n",
    "        pq_file = f'/kaggle/input/asl-signs/{d.path}'\n",
    "        #print(pq_file)\n",
    "        xyz = load_relevant_data_subset(pq_file)\n",
    "\n",
    "        output = prediction_fn(inputs=xyz)\n",
    "        p = output['outputs'].reshape(-1)\n",
    "\n",
    "        valid['sign'].append(p)\n",
    "\n",
    "        #---\n",
    "        if t%100==0:\n",
    "            time_taken = timer() - start_timer\n",
    "            print('\\r %8d / %d  %s'%(t,valid_num,time_to_str(time_taken,'sec')),end='',flush=True)\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "    truth = valid_df.label.values\n",
    "    sign  = np.stack(valid['sign'])\n",
    "    predict = np.argsort(-sign, -1)\n",
    "    correct = predict==truth.reshape(valid_num,1)\n",
    "    topk = correct.cumsum(-1).mean(0)[:5]\n",
    "\n",
    "\n",
    "    print(f'time_taken = {time_to_str(time_taken,\"sec\")}')\n",
    "    print(f'time_taken for LB = {time_taken*1000/valid_num:05f} msec\\n')\n",
    "    for i in range(5):\n",
    "        print(f'topk[{i}] = {topk[i]}')  \n",
    "    print('----- end -----\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
